{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Spam/Ham Prediction\n",
    "\n",
    "In this project, you will use what you've learned in class to create a classifier that can distinguish spam emails from ham (non-spam) emails.\n",
    "\n",
    "We'll walk you through a couple steps to get you started, but this project is almost entirely open-ended. Instead of providing you with a skeleton to fill in, we will evaluate your work based on your model's accuracy and your written responses in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle\n",
    "\n",
    "This project is a bit different from the other assignments in this class because we are using Kaggle to evaluate your model's accuracy. Kaggle is a website that hosts machine learning competitions.\n",
    "\n",
    "We've created a competition just for this project: https://www.kaggle.com/t/433a6bca95f94a78a0d2a6e7e8b311c3\n",
    "\n",
    "Here's how submitting to Kaggle works:\n",
    "\n",
    "1. You will create a classifier using the training dataset.\n",
    "2. You will use your classifier to make predictions on the test dataset.\n",
    "3. You will upload your predictions as a CSV to https://www.kaggle.com/t/433a6bca95f94a78a0d2a6e7e8b311c3\n",
    "4. The website will tell you your accuracy on the test set. You may only do this twice a day. You must reach a test set accuracy of **88%** in order to get full credit for the Kaggle portion of the assignment.\n",
    "\n",
    "(After the assignment ends, we will evaluate your accuracy on a private test set to ensure that you aren't overfitting to the test set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "This project has no ok tests (and no autograder). Instead, you will submit the following:\n",
    "\n",
    "0. **Your notebook to OkPy**. You can do this by running the `ok.submit()` cell at the bottom of this notebook. Note that there is no autograder for this assignment so you will not receive autograder emails.\n",
    "0. **Your notebook's written answers to GradeScope.** The cell to export the notebook is located at the bottom of this notebook. If you have trouble converting your notebook to PDF, you may upload your notebook to http://datahub.berkeley.edu/ and run the cell there.\n",
    "0. **Your model's predictions on the test set to Kaggle**, a website that hosts machine learning competitions. Kaggle will output your your accuracy on the test set so that you will know whether you've met the accuracy threshold or not.\n",
    "\n",
    "**To prevent you from fitting to the test set, you may only upload predictions to Kaggle twice per day.** This means you should start early. In addition, if you decide to pair with someone else, your group only gets two submissions per day (not four).\n",
    "\n",
    "This project (notebook + Gradescope submissions) is officially due Friday, Dec 1 at 11:59:59pm since we can't make assignments due after classes end. However, we will accept submissions until **Monday, Dec 4 at 11:59:59pm** without using slip days. Submissions after Dec 4 will use 1 slip day each day after Dec 4. The Kaggle competition will remain open until **Saturday Dec 9 at 11:59:59pm**.\n",
    "\n",
    "**No late Kaggle submissions will be accepted** since we've taken slip days into account when setting the Kaggle deadline. You will not use slip days for Kaggle submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "Grading will be based on a number of set criteria, enumerated below:\n",
    "\n",
    "Task | Description\n",
    "--- | ---\n",
    "Basic Classifier | You succesfully implement our guided basic logistic regression classifier.\n",
    "EDA | You create four exploratory plots that help explain your feature choices.\n",
    "Feature Selection | You explain and justify your feature selection process\n",
    "Written Questions | You answer the written questions that we place throughout this notebook.\n",
    "Kaggle Accuracy | Your model beats the prediction accuracy threshold of **88%**. This is attainable with a well-thought-out model.\n",
    "\n",
    "**You are allowed to work in groups of 2 for this assignment!** If you decide to partner with someone else, make sure you do the following:\n",
    "\n",
    "1. Have one person in the group invite the other on OkPy: https://okpy.org/cal/ds100/fa17/proj2/\n",
    "1. Have one person in the group invite the other person on Gradescope.\n",
    "1. Have one person in the group invite the other person on Kaggle: https://www.kaggle.com/t/433a6bca95f94a78a0d2a6e7e8b311c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prizes\n",
    "\n",
    "Although you need to reach 88% accuracy in order to get full credit, we will reward those that create great classifiers.\n",
    "\n",
    "The top 10 students on the Kaggle leaderboard, evaluated by their score in the private test set will: \n",
    "\n",
    "1. Have bragging rights \n",
    "2. Be invited to attend a lunch at the Faculty Club, hosted by Professors Gonzalez and Nolan.\n",
    "\n",
    "## Restrictions\n",
    "\n",
    "While we want you to be creative with your models, we want to make it fair to students who are seeing these techniques for the first time.  As such, **you are only allowed to train logistic regression models and their regularized forms**.  This means no random forest, CART, neural nets, etc.  However, you are free to feature engineer to your heart's content.  Remember that domain knowledge is the third component of data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "Assignment: Project 2\n",
      "OK, version v1.13.9\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from IPython.display import display, Latex, Markdown, HTML, Javascript\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('proj2.ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in as aviarjavalingam@berkeley.edu\n"
     ]
    }
   ],
   "source": [
    "# Log into OkPy.\n",
    "# You might need to change this to ok.auth(force=True) if you get an error\n",
    "ok.auth(force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, run this cell to highlight the written response cells in light blue. Only the highlighted cells will be converted to the GradeScope PDF, so put your written answers there.\n",
    "\n",
    "Unfortunately, you'll have to run this each time you open your notebook to highlight cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "Jupyter.notebook.get_cells().map(function(cell) {\n",
       "  var tags = cell.metadata.tags\n",
       "  if (tags && tags.indexOf('written') >= 0)\n",
       "    cell.element.css('background-color', '#efefff')\n",
       "})\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_cells = '''\n",
    "Jupyter.notebook.get_cells().map(function(cell) {\n",
    "  var tags = cell.metadata.tags\n",
    "  if (tags && tags.indexOf('written') >= 0)\n",
    "    cell.element.css('background-color', '#efefff')\n",
    "})\n",
    "'''\n",
    "display(Javascript(highlight_cells))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Data\n",
    "\n",
    "The dataset consists of email messages and their labels (0 for ham, 1 for spam). The training set contains 8348 labeled examples, and the test set contains 1000 unlabeled examples.\n",
    "\n",
    "Run the following cells to load in the data into DataFrames.\n",
    "\n",
    "The `train` DataFrame contains labeled data that you will use to train your model. It contains three columns:\n",
    "\n",
    "1. `id`: An identifier for the training example.\n",
    "1. `subject`: The subject of the email\n",
    "1. `email`: The text of the email.\n",
    "1. `spam`: 1 if the email was spam, 0 if the email was ham (not spam).\n",
    "\n",
    "The `test` DataFrame contains another set of 1000 unlabeled examples. You will predict labels for these examples and submit your predictions to Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: A&amp;L Daily to be auctioned in bankrupt...</td>\n",
       "      <td>url: http://boingboing.net/#85534171\\n date: n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: Wired: \"Stronger ties between ISPs an...</td>\n",
       "      <td>url: http://scriptingnews.userland.com/backiss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Subject: It's just too small                  ...</td>\n",
       "      <td>&lt;html&gt;\\n &lt;head&gt;\\n &lt;/head&gt;\\n &lt;body&gt;\\n &lt;font siz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Subject: liberal defnitions\\n</td>\n",
       "      <td>depends on how much over spending vs. how much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Subject: RE: [ILUG] Newbie seeks advice - Suse...</td>\n",
       "      <td>hehe sorry but if you hit caps lock twice the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            subject  \\\n",
       "0   0  Subject: A&L Daily to be auctioned in bankrupt...   \n",
       "1   1  Subject: Wired: \"Stronger ties between ISPs an...   \n",
       "2   2  Subject: It's just too small                  ...   \n",
       "3   3                      Subject: liberal defnitions\\n   \n",
       "4   4  Subject: RE: [ILUG] Newbie seeks advice - Suse...   \n",
       "\n",
       "                                               email  spam  \n",
       "0  url: http://boingboing.net/#85534171\\n date: n...     0  \n",
       "1  url: http://scriptingnews.userland.com/backiss...     0  \n",
       "2  <html>\\n <head>\\n </head>\\n <body>\\n <font siz...     1  \n",
       "3  depends on how much over spending vs. how much...     0  \n",
       "4  hehe sorry but if you hit caps lock twice the ...     0  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "# We lower case the emails to make them easier to work with\n",
    "train['email'] = train['email'].str.lower()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: CERT Advisory CA-2002-21 Vulnerabilit...</td>\n",
       "      <td>\\n \\n -----begin pgp signed message-----\\n \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: ADV: Affordable Life Insurance ddbfk\\n</td>\n",
       "      <td>low-cost term-life insurance!\\n save up to 70%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Subject: CAREER OPPORTUNITY.  WORK FROM HOME\\n</td>\n",
       "      <td>------=_nextpart_000_00a0_03e30a1a.b1804b54\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Subject: Marriage makes both sexes happy\\n</td>\n",
       "      <td>url: http://www.newsisfree.com/click/-3,848315...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Subject: Re: [SAtalk] SA very slow (hangs?) on...</td>\n",
       "      <td>on thursday 29 august 2002 16:39 cet mike burg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            subject  \\\n",
       "0   0  Subject: CERT Advisory CA-2002-21 Vulnerabilit...   \n",
       "1   1    Subject: ADV: Affordable Life Insurance ddbfk\\n   \n",
       "2   2     Subject: CAREER OPPORTUNITY.  WORK FROM HOME\\n   \n",
       "3   3         Subject: Marriage makes both sexes happy\\n   \n",
       "4   4  Subject: Re: [SAtalk] SA very slow (hangs?) on...   \n",
       "\n",
       "                                               email  \n",
       "0  \\n \\n -----begin pgp signed message-----\\n \\n ...  \n",
       "1  low-cost term-life insurance!\\n save up to 70%...  \n",
       "2  ------=_nextpart_000_00a0_03e30a1a.b1804b54\\n ...  \n",
       "3  url: http://www.newsisfree.com/click/-3,848315...  \n",
       "4  on thursday 29 august 2002 16:39 cet mike burg...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test['email'] = test['email'].str.lower()\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "In the cell below, print the text of the first ham and the first spam email in the training set. Then, discuss one thing you notice that is different between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": [
     "written",
     "q01",
     "student"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: http://boingboing.net/#85534171\n",
      " date: not supplied\n",
      " \n",
      " arts and letters daily, a wonderful and dense blog, has folded up its tent due \n",
      " to the bankruptcy of its parent company. a&l daily will be auctioned off by the \n",
      " receivers. link[1] discuss[2] (_thanks, misha!_)\n",
      " \n",
      " [1] http://www.aldaily.com/\n",
      " [2] http://www.quicktopic.com/boing/h/zlfterjnd6jf\n",
      " \n",
      " \n",
      "\n",
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      " <font size=3d\"4\"><b> a man endowed with a 7-8\" hammer is simply<br>\n",
      "  better equipped than a man with a 5-6\"hammer. <br>\n",
      " <br>would you rather have<br>more than enough to get the job done or fall =\n",
      " short. it's totally up<br>to you. our methods are guaranteed to increase y=\n",
      " our size by 1-3\"<br> <a href=3d\"http://209.163.187.47/cgi-bin/index.php?10=\n",
      " 004\">come in here and see how</a>\n",
      " </body>\n",
      " </html>\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "*Type your answer here, replacing this text.*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the text of the first ham and the first spam emails. Then, fill in your response in the q01 variable:\n",
    "\n",
    "print(train[train['spam'] == 0].reset_index().email[0])\n",
    "print(train[train['spam'] == 1].reset_index().email[0])\n",
    "\n",
    "q01 = '''\n",
    "*Type your answer here, replacing this text.*\n",
    "'''\n",
    "display(Markdown(q01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Features\n",
    "\n",
    "We would like to take the text of an email and predict whether the text is ham or spam. This is a *classification* problem, so we will use logistic regression to make a classifier.\n",
    "\n",
    "Recall that the input to logistic regression is a matrix $X$ that contains numeric values only. Unfortunately, our data are text, not numbers. To address this, we can create numeric features derived from the email text and use those features for logistic regression.\n",
    "\n",
    "Each row of $X$ is derived from one email example. Each column of $X$ is one feature. We'll guide you through creating a simple feature, and you'll create more interesting ones when you are trying to increase your accuracy.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Create a function called `words_in_text` that takes in a list of words and the text of an email. It outputs a pandas Series containing either a 0 or a 1 for each word in the list. The value of the Series should be 0 if the word doesn't appear in the text and 1 if the word does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def words_in_text(words, text):\n",
    "    '''\n",
    "    Args:\n",
    "        `words` (list of str): words to find\n",
    "        `text` (str): string to search in\n",
    "    \n",
    "    Returns:\n",
    "        Series containing either 0 or 1 for each word in words\n",
    "        (0 if the word is not in text, 1 if the word is).\n",
    "    '''\n",
    "    \n",
    "    dic = np.array([])\n",
    "    for word in words:\n",
    "        if word in text:\n",
    "            dic = np.append(dic, 1)\n",
    "        else:\n",
    "            dic = np.append(dic, 0)\n",
    "    return dic\n",
    "\n",
    "# If these don't error, your function outputs the correct output for these examples\n",
    "assert np.allclose(words_in_text(['hello'], 'hello world'),\n",
    "                   [1])\n",
    "assert np.allclose(words_in_text(['hello', 'bye', 'world'], 'hello world hello'),\n",
    "                   [1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Now, create a function called `words_in_texts` that takes in a list of words and a pandas Series of email texts. It should output a 2-dimensional NumPy matrix containing one row for each email text. The row should contain the output of `words_in_text` for each example. For example:\n",
    "\n",
    "```python\n",
    ">>> words_in_texts(['hello', 'bye', 'world'], pd.Series(['hello', 'hello world hello']))\n",
    "array([[1, 0, 0],\n",
    "       [1, 0, 1]])\n",
    "```\n",
    "\n",
    "You should be able to use the `.apply` and `.as_matrix` functions to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def words_in_texts(words, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        `words` (list of str): words to find\n",
    "        `texts` (Series of str): strings to search in\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array of 0s and 1s with shape (n, p) where n is the\n",
    "        number of texts and p is the number of words.\n",
    "    '''\n",
    "    lst = []\n",
    "    for text in texts:\n",
    "        lst = np.append(lst, words_in_text(words, text)) \n",
    "    return lst.reshape(len(texts), len(words))\n",
    "\n",
    "# If these don't error, your function outputs the correct output for these examples\n",
    "assert np.allclose(words_in_texts(['hello', 'bye', 'world'], pd.Series(['hello', 'hello world hello'])),\n",
    "                   np.array([[1, 0, 0], [1, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Notice that the output of `words_in_texts` is a numeric matrix containing features for each email. This means we can use it directly to train a classifier.\n",
    "\n",
    "### Question 4\n",
    "\n",
    "We've given you 5 words that might be useful as features to distinguish spam/ham emails. Use these words as well as the `train` DataFrame to create two NumPy arrays: `X_train` and `y_train`.\n",
    "\n",
    "`X_train` should be a matrix of 0s and 1s created by using your `words_in_texts` function on all the emails in the training set.\n",
    "\n",
    "`y_train` should be vector of the correct labels for each email in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true,
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.]]), [0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_words = ['drug', 'bank', 'prescription', 'memo', 'private']\n",
    "\n",
    "X_train = words_in_texts(some_words, train.email)\n",
    "y_train = [x for x in train['spam']]\n",
    "\n",
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Now we have matrices we can give to scikit-learn! Using the [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier, train a logistic regression model using `X_train` and `y_train`. Then, output the accuracy of the model in the cell below. You should get an accuracy of around 0.7557."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75574988021082889"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import zero_one_loss\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "Y_hat = clf.predict(X_train)\n",
    "error = zero_one_loss(y_train, Y_hat)\n",
    "accuracy = 1 - error\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "That doesn't seem too shabby! But the classifier you made above isn't as great as you might think. Recall that we have other ways of evaluating a classifier:\n",
    "\n",
    "*Sensitivity* (also called *recall*) is the rate of true positives; in this case, the proportion of spam emails that are classified as spam.\n",
    "\n",
    "*Specificity* (also called *precision*) is the rate of true negatives; in this case, the proportion of ham emails that are classified as ham.\n",
    "\n",
    "Answer the following questions in the light blue cell below. You may create other cells for scratch work, but your final answers\n",
    "must appear in the light blue cell.\n",
    "\n",
    "0. Suppose we have a classifier that just predicts 0 (ham) for every email. What is its sensitivity? Its specificity?\n",
    "0. Suppose we have a classifier that just predicts 0 (ham) for every email. What is its accuracy on the training set?\n",
    "0. Our logistic regression classifier got 75% prediction accuracy (number of correct predictions / total). Why is this a poor accuracy?\n",
    "0. What is the sensitivity of the logistic regression classifier above? The specificity? What kind of mistake is our classifier more likely to make: false positives or false negatives?\n",
    "0. Given the word features we gave you above, name one reason this classifier is performing poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8348"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6208"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['spam'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2140"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[train['spam'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8348"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "always_ham = [0 for i in range(8348)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74365117393387636"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - zero_one_loss(y_train, always_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.63466666666666671, 0.11121495327102804, 0.18926441351888668, None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_train, Y_hat, average = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student",
     "written",
     "q06"
    ]
   },
   "source": [
    "0. The recall of this predictor would be 0 while the precision of this same predictor would be 0 or N/A (depending on who you ask).\n",
    "0. The accuracy of a predictor that always returns 0 (HAM) is 74.365%.\n",
    "0. 75% is an outstandingly poor accuracy because it is only one percent better than a predictor that simply returns 0 no matter what, which is virtually ignorable. \n",
    "0. The recall of the classifier is .1112 while the precision is .6347. Our classifier is much more likely to make a false negative than a false positive because there were 375 positives and 7973 negatives.\n",
    "0. One possible explanation for why our predictor is performing so poorly with the given word features is that the words are all english words that are lower cased and related to each other with no inclination that the words belong specifically to spam or ham. This is the rough equivalent of asking someone to classify something as a fruit or not a fruit by giving them the information of a bunch of orange things and telling them which ones are fruits and which ones aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Forward\n",
    "\n",
    "With this in mind, it is now your assignment to make your classifier more accurate. In particular, in order to get full credit on the accuracy part of this assignment, you must get at least **88%** accuracy on the test set. To see your accuracy on the test set, you will use your classifier to predict every email in the `test` DataFrame and upload your predictions to Kaggle.\n",
    "\n",
    "To prevent you from fitting to the test set, you may only upload predictions to Kaggle twice per day. This means you should start early!\n",
    "\n",
    "Here are some ideas for improving your model:\n",
    "\n",
    "1. Finding better features based on the email text. For example, simple features that typically work for emails are:\n",
    "    1. Number of characters in the subject / body\n",
    "    1. Number of words in the subject / body\n",
    "    1. Use of punctuation (e.g., how many '!' were there?)\n",
    "    1. Number / percentage of capital letters \n",
    "    1. Whether or not the email is a reply to an earlier email or a forwarded email. \n",
    "    1. Using bag-of-words or [td-idf](http://www.tfidf.com/).\n",
    "1. Finding better words to use as features. Which words are the best at distinguishing emails? This requires digging into the email text itself. (To help you out, we've given you a set of [English stopwords](https://www.wikiwand.com/en/Stop_words) in `stopwords.csv`)\n",
    "1. Better data processing. For example, many emails contain HTML as well as text. You can consider extracting out the text from the HTML to help you find better words. Or, you can match HTML tags themselves, or even some combination of the two.\n",
    "1. Model selection. You can adjust parameters of your model (e.g., the regularization parameter) to achieve higher accuracy. \n",
    "\n",
    "Recall that you should use cross-validation to do feature and model selection properly! Otherwise, you will likely overfit to your training data.\n",
    "\n",
    "You may use whatever method you prefer in order to create features. However, we want to make it fair to students who are seeing these techniques for the first time.  As such, **you are only allowed to train logistic regression models and their regularized forms**. This means no random forest, k-nearest-neighbors, neural nets, etc.\n",
    "\n",
    "We will not give you a code skeleton to do this, so feel free to create as many cells as you need in order to tackle this task. However, you should show us your process as outlined here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature/Model Selection Process\n",
    "\n",
    "In this following cell, describe the process of improving your model. You should use at least 2-3 sentences each to address the follow questions:\n",
    "\n",
    "1. How did you find better features for your model?\n",
    "2. What did you try that worked / didn't work?\n",
    "3. What was surprising in your search for good features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student",
     "written",
     "q_feature"
    ]
   },
   "source": [
    "1. *Write your answer here, replacing this text.*\n",
    "1. *Write your answer here, replacing this text.*\n",
    "1. *Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "In the four light blue cells below, show us four different visualizations that you used to select features for your model. Each cell should output:\n",
    "\n",
    "1. A plot showing something meaningful about the data that helped you during feature / model selection.\n",
    "2. 2-3 sentences describing what you plotted and what its implications are for your features.\n",
    "\n",
    "Feel to create as many plots as you want in your process of feature selection, but select four interesting ones for the cells below.\n",
    "\n",
    "You should not show us more than one visualization for the same type of feature. For example, don't show us a bar chart of the number of emails that contain the word \"hello\" and a bar chart of the number of emails that contain the word \"world\". Each visualization should be conceptually distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_df = pd.read_csv('stopwords.csv', sep=',',header=None)\n",
    "stopwords = set(stopwords_df[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def unique_word_frequency(string_series):\n",
    "    word_dict = {}\n",
    "    freq = {}\n",
    "    length = string_series.size\n",
    "    for ind_string in string_series:\n",
    "        words = set(((re.sub('\\w*\\d\\w*', ' ',(re.sub('<[^<]+?>', ' ', ind_string.lower())))).translate(str.maketrans('','',string.punctuation))).split())\n",
    "        for word in words:\n",
    "            if word not in stopwords:\n",
    "                if word in word_dict:\n",
    "                    word_dict[word] = 1 + word_dict[word]\n",
    "                else:\n",
    "                    word_dict[word] = 1\n",
    "    for key in word_dict:\n",
    "        if ((len(key) < 16) and (len(key) > 2)):\n",
    "            if word_dict[key] > 0:\n",
    "                freq[key] = word_dict[key] / length\n",
    "    \n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_freq = unique_word_frequency(train[train['spam'] == 0]['email'])\n",
    "spam_freq = unique_word_frequency(train[train['spam'] == 1]['email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_spam_scorer(ham_dict, spam_dict):\n",
    "    word_score_dict = {}\n",
    "    for key in spam_dict:\n",
    "        word_given_spam = spam_dict[key]\n",
    "        word_given_ham = 0\n",
    "        if key in ham_dict:\n",
    "            word_given_ham = ham_dict[key]\n",
    "        spam_score = word_given_spam / (word_given_spam + word_given_ham)\n",
    "        word_score_dict[key] = spam_score\n",
    "    return word_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_score = naive_bayes_spam_scorer(ham_freq, spam_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame.from_dict(sorted(nb_score.items(), key=lambda x:x[1]))\n",
    "scores = scores[scores[1] < 1]\n",
    "#scores = scores[scores[1] > 0.95]\n",
    "#scores.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_score_ham = naive_bayes_spam_scorer(spam_freq, ham_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_ham = pd.DataFrame.from_dict(sorted(nb_score_ham.items(), key=lambda x:x[1]))\n",
    "scores_ham = scores_ham[scores_ham[1] < 1]\n",
    "#scores_ham = scores_ham[scores_ham[1] > 0.95]\n",
    "#scores_ham.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    return text.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conc(text_array):\n",
    "    return ' '.join(text_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['email_smooth'] = ((((((train['email'].str).lower()).str.replace(r'<[^<]+?>', ' ')).str.replace(r'\\w*\\d\\w*', ' ')).apply(remove_punctuations)).str.split()).apply(conc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words = \"english\", smooth_idf = False)\n",
    "X = tfidf.fit_transform(train.email_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scores.to_csv('spam_email_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scores_ham.to_csv('mam_email_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_array_words = np.array(['requested', 'trading', 'profits', 'shipped', 'fortunate', 'utah', 'ohio', 'align', 'responded', 'livelihood', 'mccoy', 'drawings', 'imagined', 'savvy', 'seminars', 'respectability', 'trump', 'richmond', 'ini', 'informix', 'romania', 'labeling', 'cor', 'eligibility', 'occassion', 'calvin', 'meyers', 'obligations', 'earnest', 'webs', 'redesign', 'pounds', 'ally', 'ions', 'materially', 'specialized', 'reward', 'melt', 'sweets', 'deducted', 'dayton', 'earns', 'sunrise', 'labelled', 'shocking', 'backpack', 'quicker', 'resellers', 'vegas', 'casey', 'thanx', 'boyfriend', 'lion', 'diabetic', 'omen', 'swiftly', 'demise', 'specialty', 'unheard', 'delighted', 'instore', 'unforgettable', 'coli', 'deferred', 'deliveries', 'wines', 'waiters', 'marble', 'gigantic', 'turks', 'pierre', 'caledonia', 'nepal', 'marina', 'negotiating', 'poker', 'organize', 'webcam', 'notices', 'expire', 'cram', 'jelly', 'officejet', 'ixs', 'lump', 'xerox', 'nbspfor', 'catherine', 'dvdr', 'amateurs', 'sanitary', 'quotin', 'enhance', 'iid', 'iiip', 'iip', 'sixteen', 'bedside', 'diets', 'magnifying', 'ethics', 'quicken', 'duplication', 'shipment', 'graduate', 'construct', 'necessities', 'tuition', 'booming', 'unblocked', 'tidal', 'respectfully', 'roth', 'informationquot', 'appologise', 'thicker', 'decreases', 'hisher', 'juillet', 'modifiable', 'cela', 'hugo', 'denis', 'septembre', 'votre', 'quip', 'dun', 'plateforme', 'detective', 'antiques', 'multiplayer', 'shuffling', 'porno', 'ling', 'perpetrating', 'misconceptions', 'elders', 'neighborhoods', 'synonymous', 'reemerged', 'brethren', 'embarrassment', 'contingency', 'inhabit', 'negativity', 'thirtythree', 'mandatory', 'selfishness', 'wishing', 'voicestream', 'laundering', 'imprisonment', 'playstation', 'compresses', 'roundtrip', 'recycling', 'tang', 'comm', 'tien', 'jacqueline', 'landlord', 'unjustified', 'preprogrammed', 'mohammed', 'celeb', 'applegate', 'dipping', 'exhibits', 'layouts', 'unblocking', 'backgrounds', 'hindering', 'wallets', 'prudential', 'disability', 'redeem', 'redeemable', 'holland', 'multiplatform', 'sacred', 'erroneous', 'burners', 'selfesteem', 'intensity', 'maui', 'promotes', 'hybrid', 'anxiety', 'orgasm', 'partake', 'implication', 'inquire', 'calmer', 'conferencing', 'dana', 'clan', 'vise', 'apy', 'heal', 'fatigue', 'rests', 'guage', 'visits', 'lighting', 'girth', 'knocking', 'heures', 'calltime', 'ahi', 'ada', 'shortening', 'informa', 'fcfd', 'waived', 'broadway', 'thestreetcom', 'dialling', 'judicial', 'acquaintance', 'owne', 'confines', 'mutually', 'moles', 'pobox', 'cupertino', 'thames', 'internationally', 'gre', 'crowe', 'flyer', 'nocost', 'ceaseanddesist', 'reelection', 'unthinkable', 'presidentceo', 'dormitory', 'parkinsons', 'maximizer', 'survivors', 'workathome', 'bordertop', 'borderright', 'iiutaintorg', 'borderbottom', 'sharpen', 'monte', 'stamps', 'yada', 'direkt', 'dsseldorf', 'warsaw', 'mohammad', 'diagnosis', 'fountain', 'sundays', 'remuneration', 'gel', 'qvc', 'ssg', 'reel', 'disheartening', 'arthritis', 'battles', 'climates', 'tacking', 'sussex', 'naomi', 'forefront', 'nurturing', 'staffed', 'unsatisfied', 'calcium', 'arteries', 'stimulant', 'contractions', 'organs', 'colon', 'sugars', 'testes', 'mugging', 'crowded', 'knives', 'tradeshow', 'exhibitions', 'exercised', 'accountants', 'mems', 'psp', 'intervened', 'anne', 'haben', 'allergies', 'cons', 'jubii', 'fireball', 'suchen', 'austrian', 'oomph', 'messengers', 'natasha', 'physician', 'msword', 'rea', 'alloy', 'pcp', 'splendor', 'baggage', 'modesty', 'waiver', 'ailable', 'garrison', 'depot', 'appreciates', 'upc', 'touchscreen', 'woodworking', 'poss', 'collectible', 'treadmill', 'callcenter', 'crw', 'interne', 'establishes', 'brewed', 'searchable', 'interfere', 'reprisal', 'hindrance', 'neu', 'resin', 'radiant', 'foam', 'otto', 'phaseout', 'middleton', 'extr', 'offer', 'loss', 'telephone', 'contacts', 'thank', 'financial', 'lose', 'congratulations', 'professionals', 'predict', 'lawful', 'advisor', 'internets', 'teen', 'relax', 'shall', 'fraction', 'rates', 'formula', 'processed', 'comfort', 'newsgroups', 'multipart', 'approved', 'motivated', 'alaska', 'amateur', 'preparation', 'ent', 'mandate', 'superior', 'buyers', 'onetime', 'thirty', 'epson', 'pics', 'fontsize', 'confidential', 'hundreds', 'explosive', 'wyoming', 'tennessee', 'filling', 'inkjet', 'operated', 'prestigious', 'hype', 'sheets', 'unhappy', 'verification', 'ion', 'boundary', 'retail', 'directed', 'lifetime', 'fortune', 'dakota', 'hidden', 'diet', 'luckily', 'entertained', 'litigation', 'merchant', 'proven', 'regarding', 'wrapping', 'concealed', 'valued', 'wholesale', 'risk', 'vermont', 'surgery', 'refreshing', 'classmates', 'associate', 'kingdom', 'handbook', 'stroke', 'exp', 'formulas', 'projection', 'draws', 'blonde', 'opt', 'gambling', 'nbsp', 'unsolicited', 'eliminated', 'estate', 'signup', 'housing', 'remove', 'offers', 'illinois', 'guarantees', 'auto', 'inhouse', 'toners', 'invaluable', 'void', 'bank', 'http', 'millionaires', 'marketer', 'communicator', 'singles', 'quickest', 'adviser', 'traced', 'bless', 'puerto', 'nude', 'specifics', 'arial', 'sought', 'slice', 'colony', 'gen', 'humbly', 'vista', 'magnificent', 'fund', 'easytouse', 'com', 'spouse', 'par', 'repairs', 'absolutely', 'xxx', 'entitled', 'residents', 'wisconsin', 'ezine', 'authorize', 'contracts', 'qualification', 'inclusive', 'wit', 'indexhtml', 'colleagues', 'orders', 'envelope', 'employment', 'fred', 'traps', 'custody', 'savings', 'lessons', 'improvement', 'websites', 'texthtml', 'dear', 'nbspnbspnbsp', 'endowed', 'inspires', 'sluts', 'cock', 'spray', 'utilizing', 'serenity', 'recieve', 'ect', 'verifiable', 'receptive', 'disposal', 'cafeteria', 'paving', 'snippet', 'stuffy', 'retire', 'quebec', 'freak', 'gourmet', 'bankers', 'hungary', 'barbados', 'cape', 'slovakia', 'bermuda', 'slovenia', 'zurich', 'apprehensive', 'servernbsp', 'disconnected', 'simulates', 'midsize', 'iceberg', 'rape', 'smalltime', 'grind', 'retails', 'issuer', 'yearly', 'plight', 'convey', 'turbo', 'omissions', 'voted', 'webmasters', 'eyeopening', 'gentle', 'imagegif', 'vic', 'testdrive', 'commute', 'doctorate', 'physiological', 'amazes', 'lebanese', 'sinew', 'acclaimed', 'unserem', 'unanimously', 'exile', 'interior', 'impartial', 'seas', 'affiliated', 'fontfamily', 'affordable', 'amazed', 'accepted', 'advertisement', 'exercise', 'charsetiso', 'beneficial', 'instruments', 'safely', 'urgent', 'shipping', 'louisiana', 'multi', 'banners', 'pledge', 'momentum', 'interrupted', 'obtaining', 'sensitivity', 'integral', 'investigated', 'advertisements', 'traded', 'subscriber', 'investment', 'satisfaction', 'proposal', 'extracted', 'fourteen', 'congestion', 'bedroom', 'nbspand', 'adults', 'schoolgirls', 'deceased', 'vip', 'conditional', 'residing', 'identifies', 'solicitation', 'interacting', 'exhibition', 'strongly', 'trillion', 'giveaway', 'opportunity', 'ordering', 'promotions', 'satisfied', 'anytime', 'protective', 'bsp', 'investigative', 'extracting', 'weapon', 'vos', 'tablets', 'refund', 'incredible', 'reply', 'ffffff', 'jennifer', 'compete', 'visitors', 'favour', 'procedures', 'cabinet', 'purchase', 'overnight', 'bonus', 'appetite', 'recruiting', 'gamble', 'repair', 'magazines', 'mailin', 'zealand', 'zip', 'jersey', 'warranty', 'emails', 'qualify', 'sum', 'cash', 'valuable', 'rhode', 'hawaii', 'kansas', 'cede', 'exploding', 'chemically', 'guidelines', 'ers', 'manuals', 'awe', 'clic', 'multilingual', 'awaiting', 'clicking', 'exchanging', 'legality', 'enjoyable', 'disregard', 'mil', 'supplements', 'wellbeing', 'ammunition', 'bas', 'analyzes', 'accrued', 'paraguay', 'fiji', 'bangladesh', 'botswana', 'saint', 'norfolk', 'somalia', 'lebanon', 'ergonomic', 'naughty', 'invoicing', 'complies', 'peln', 'ime', 'smut', 'manufacture', 'nbspwe', 'mines', 'sansserif', 'ven', 'gua', 'hav', 'systemnbsp', 'constructing', 'simplest', 'unpredictable', 'tutoring', 'destiny', 'fulfill', 'ambition', 'finalized', 'pendant', 'pouvez', 'contrat', 'firmer', 'planners', 'unlock', 'overworked', 'eve', 'drowning', 'contracting', 'collectors', 'natures', 'lin', 'photographic', 'ahebaxeb', 'passports', 'mlms', 'lopez', 'angioplasty', 'invasive', 'transplant', 'sunglasses', 'flown', 'helicopters', 'newark', 'gum', 'pheromone', 'dosages', 'cortex', 'extravagantly', 'concurrently', 'fructus', 'issuance', 'prescriptions', 'organism', 'billie', 'aaliyah', 'leisure', 'personals', 'turk', 'mugabe', 'embassy', 'aad', 'mth', 'proportions', 'peers', 'parttime', 'prnewswire', 'hourly', 'embark', 'relocating', 'ali', 'pls', 'factored', 'stoner', 'shun', 'brutal', 'sybase', 'foxpro', 'cotton', 'corey', 'wnt', 'vrs', 'ausgabe', 'dressing', 'headset', 'cuz', 'inlaid', 'contravention', 'rmi', 'exert', 'holdover', 'prevailed', 'interred', 'denounced', 'timehonored', 'sanctuaries', 'nineteenth', 'steadfastly', 'terminates', 'proclaiming', 'tacitly', 'reprehensible', 'unchallenged', 'godgiven', 'ceremonies', 'defamation', 'atoll', 'contemplated', 'implausible', 'embodied', 'turtles', 'responds', 'deception', 'remnant', 'bibliography', 'evasion', 'cognizant', 'refugees', 'maliciously', 'apathy', 'unobstructed', 'disputed', 'insatiable', 'raison', 'hereditary', 'ominous', 'heretofore', 'pantry', 'belligerence', 'uninhabited', 'proudly', 'inflammatory', 'predates', 'monarchy', 'alchemy', 'parting', 'unjustifiably', 'qualifications', 'millionaire', 'inexpensive', 'susan', 'warehouse', 'lending', 'wish', 'transaction', 'prey', 'placing', 'stepbystep', 'helvetica', 'transmissions', 'presentations', 'invest', 'weight', 'dreaming', 'lowering', 'owning', 'shocked', 'commission', 'creditors', 'emailed', 'muscle', 'lowcost', 'prescribed', 'treasury', 'succeeding', 'gifts', 'ver', 'pharmaceutical', 'dollars', 'guarantee', 'miami', 'blaster', 'sierra', 'cigarettes', 'fantasies', 'reps', 'plates', 'profession', 'impulse', 'insured', 'concealing', 'marketing', 'communication', 'instruction', 'destructive', 'obtained', 'inform', 'credit', 'fundamentals', 'stylus', 'pill', 'income', 'bull', 'historically', 'awarded', 'greedy', 'inquiries', 'reduction', 'exceed', 'scholarship', 'endeavor', 'pencil', 'sized', 'defective', 'snoop', 'seventy', 'enhancers', 'stimulate', 'midwest', 'nbspa', 'crawl', 'coral', 'hormone', 'billed', 'filthy', 'postman', 'jewelry', 'attitudes', 'booster', 'lifes', 'bio', 'unconditional', 'phoenix', 'sunk', 'martial', 'cerebral', 'cpa', 'diagnose', 'unwanted', 'packard', 'receive', 'promptly', 'diplomatic', 'consolidation', 'debt', 'unlimited', 'arkansas', 'housekeepers', 'jour', 'mandated', 'cuttingedge', 'removed', 'click', 'classified', 'visa', 'platinum', 'marketers', 'soliciting', 'categorized', 'lowest', 'ref', 'equity', 'introductory', 'honored', 'medical', 'expenses', 'teasing', 'corny', 'esq', 'inflation', 'recession', 'reprint', 'dorm', 'reconsideration', 'quotas', 'aerospace', 'obey', 'substances', 'envelops', 'rode', 'closet', 'approximate', 'lubricant', 'recognizing', 'tailored', 'ins', 'quotthe', 'governmentthe', 'disadvantages', 'retina', 'reunion', 'caicos', 'kuwait', 'domination', 'babes', 'poors', 'petite', 'para', 'por', 'mas', 'creditor', 'humiliate', 'magenta', 'predicament', 'offsetting', 'fumble', 'shortest', 'recieved', 'sest', 'devis', 'ils', 'personnes', 'extranet', 'parcs', 'portail', 'patron', 'ensemble', 'axe', 'nationale', 'galement', 'dirigeant', 'nombreux', 'nurtured', 'exclusion', 'factoring', 'logistics', 'cheque', 'servant', 'bspnbsp', 'celebrities', 'smoked', 'insomnia', 'responsiveness', 'brewing', 'lust', 'cannabis', 'suppression', 'communion', 'anybodys', 'ana', 'rad', 'healing', 'gland', 'receivables', 'remit', 'relieve', 'dominion', 'bae', 'shorten', 'foregoing', 'persecution', 'incarceration', 'diners', 'superfast', 'zerocost', 'informat', 'mugabes', 'adress', 'rejuvenate', 'samplers', 'nausea', 'tin', 'exhibitors', 'handsomely', 'rebel', 'tnt', 'embraced', 'afb', 'elevated', 'monthly', 'traders', 'blueprint', 'checklist', 'preschool', 'webmaketalk', 'uncommon', 'participate', 'ext', 'dealer', 'earning', 'exciting', 'therapy', 'fees', 'payments', 'opted', 'incurred', 'screening', 'dreamed', 'nevada', 'interruption', 'virtue', 'disappearance', 'procurement', 'prospective', 'tremendous', 'est', 'deposited', 'proved', 'unbelievable', 'nationwide', 'payment', 'premium', 'mitchell', 'correspondence', 'promotion', 'bills', 'ingredients', 'loosing', 'georgia', 'multibillion', 'merchandise', 'billing', 'smokes', 'lace', 'reputable', 'uce', 'rem', 'automobile', 'contentid', 'cordially', 'formulation', 'utilized', 'vacations', 'dragon', 'toll', 'advised', 'cartridge', 'obligation', 'excluded', 'sincerely', 'affiliate', 'alink', 'apologise', 'researching', 'viagra', 'href', 'riskfree', 'specialists', 'usd', 'expiration', 'permanently', 'rental', 'receipt', 'inconvenience', 'cooperation', 'upfront', 'bordercolor', 'jam', 'blvd', 'participation', 'reclaim', 'invests', 'nownbsp', 'mortgages', 'wiretaps', 'trinidad', 'intimidate', 'juncture', 'gro', 'visor', 'mania', 'harvested', 'upandrunning', 'ses', 'donn', 'deux', 'systme', 'cas', 'approvals', 'vcd', 'owing', 'artery', 'coronary', 'enrollment', 'lungs', 'hawaiian', 'ther', 'youthful', 'tits', 'rancho', 'unqualified', 'ber', 'downsizing', 'pastor', 'discounted', 'quotwe', 'frankie', 'sweetness', 'spa', 'eed', 'faqs', 'deeds', 'cleansing', 'earn', 'loan', 'presently', 'adult', 'insurance', 'bureau', 'hewlett', 'alberta', 'breasts', 'advertisers', 'discoveries', 'hormones', 'postage', 'montana', 'economical', 'hyperlinks', 'safekeeping', 'contracted', 'mammoth', 'dental', 'commence', 'tomorrows', 'lease', 'lodge', 'dysfunction', 'secrets', 'qualified', 'sleep', 'pride', 'christian', 'mailto', 'bonuses', 'dare', 'assist', 'exceedingly', 'instructed', 'surrender', 'resumes', 'specialize', 'dialing', 'surcharges', 'formulated', 'referral', 'heighten', 'currencies', 'overweight', 'transcripts', 'discrete', 'bahamas', 'rake', 'savenbsp', 'epl', 'settlements', 'eligible', 'containers', 'himher', 'glossary', 'childcare', 'fares', 'checklists', 'underemployed', 'anxious', 'stringent', 'emailer', 'payperview', 'nous', 'entre', 'avant', 'tous', 'exwife', 'indictments', 'incidental', 'favorable', 'imagejpeg', 'specializing', 'faxing', 'num', 'dealers', 'heavenly', 'mellow', 'twentyone', 'lauderdale', 'dependable', 'coltd', 'winwin', 'orton', 'staggering', 'homeowners', 'cli', 'fortunes', 'sir', 'promotional', 'hobby', 'residual', 'penis', 'forwardlooking', 'bra', 'qualifying', 'belongings', 'ove', 'apologize', 'deposit', 'nigeria', 'crammed', 'assistance', 'utmost', 'prescription', 'consultation', 'idaho', 'explode', 'mentor', 'haunt', 'cholesterol', 'ies', 'sev', 'delegated', 'medication', 'showcases', 'lean', 'associations', 'destinations', 'herb', 'attracting', 'cart', 'bachelors', 'postmarked', 'infact', 'borderleft', 'blessings', 'quotation', 'financially', 'optin', 'toner', 'unlisted', 'profitable', 'amex', 'sirmadam', 'negotiate', 'physicians', 'strictest', 'groceries', 'loans', 'unsubscribed', 'accountant', 'topoftheline', 'tobacco', 'cbs', 'motivating', 'richer', 'commissions', 'fedex', 'fornbsp', 'offshore', 'futur', 'healthier', 'reciept', 'zzzzexamplecom', 'looseleaf', 'oneofakind', 'currency', 'quot', 'condone', 'laurent', 'avisited', 'removal', 'compliance', 'ministry', 'supervision', 'cote', 'envelopes', 'casino', 'brokers', 'thermal', 'capitalize', 'secretly', 'tournaments', 'professions', 'amalgamated', 'botanical', 'estates', 'cellpadding', 'cellspacing', 'illegality', 'ahover', 'refining', 'unclaimed', 'laserjet', 'blanks', 'guaranteed', 'seeker', 'debts', 'itnbsp', 'tout', 'debtors', 'deposits', 'removehtml', 'lesbian', 'charset', 'nbc', 'reversing', 'erase', 'teens', 'underline', 'aux', 'ger', 'nos', 'noninvasive', 'postal', 'suv', 'attn', 'pam', 'profiled', 'beneficiary', 'requesting', 'prohibiting', 'lbs', 'climax', 'ratios', 'tenants', 'nurse', 'gasoline', 'repaid', 'counseling', 'stimulating', 'specials', 'thi', 'affiliates', 'wrinkles', 'bodys', 'barrister', 'erection', 'daycare', 'appliances', 'hardcore', 'aactive', 'nationally', 'supplement', 'athletes', 'leasing', 'stamina', 'confidentiality', 'untitled', 'urgently', 'legible', 'sur', 'miracle', 'invitations', 'personalized', 'ofcourse', 'diagnostics', 'extracts', 'astonishment', 'icann', 'honesty', 'aging', 'familys', 'kindly', 'overlook', 'reentering', 'dqog', 'substance', 'spouting', 'homeowner', 'aolcom', 'solicit', 'mlm', 'resell', 'professionally', 'modalities', 'factual', 'dqogicag', 'tollfree', 'madam', 'potency', 'lis', 'systemworks', 'paperwork', 'mastercard', 'originator', 'consolidate', 'charsetwindows', 'mailings', 'bottles', 'refinance', 'moneyback', 'herbal', 'mortgage', 'optout', 'lenders'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_array_words = np.array(['monday', 'sentence', 'procedure', 'workers', 'stops', 'tagged', 'gap', 'issues', 'integration', 'recording', 'iraq', 'images', 'built', 'knows', 'storage', 'aimed', 'extension', 'animated', 'evidence', 'mirror', 'band', 'alan', 'dynamically', 'alpha', 'strings', 'wells', 'rendered', 'literature', 'weapons', 'keen', 'clutter', 'partly', 'upper', 'acceleration', 'enforce', 'bright', 'functions', 'institute', 'early', 'tells', 'cool', 'downloads', 'hollywood', 'recipe', 'complain', 'retrieving', 'hat', 'interesting', 'fell', 'upgraded', 'chirac', 'scripts', 'stripped', 'drinks', 'firewire', 'deemed', 'italian', 'outlets', 'manipulation', 'bryan', 'formatting', 'dice', 'lan', 'loses', 'storm', 'depressed', 'opponents', 'peer', 'prevented', 'organizing', 'detection', 'variant', 'dominate', 'submissions', 'scientist', 'believer', 'crimes', 'oldest', 'encourages', 'rings', 'glory', 'yamaha', 'bose', 'terror', 'atlantic', 'desert', 'defeat', 'tony', 'wisdom', 'rocks', 'resolution', 'driver', 'adds', 'linked', 'religious', 'shouldnt', 'productive', 'speakers', 'killing', 'theyve', 'runs', 'brian', 'hits', 'myth', 'economist', 'dsa', 'column', 'smoother', 'soldiers', 'cares', 'diseases', 'hub', 'physics', 'micro', 'jokes', 'javamailroot', 'thatll', 'digging', 'deeply', 'username', 'occasionally', 'mad', 'assumed', 'backs', 'van', 'direction', 'devices', 'outlook', 'desktop', 'disks', 'hopefully', 'messageid', 'speaking', 'aaron', 'worm', 'novel', 'shy', 'feed', 'permits', 'football', 'broken', 'swing', 'juniper', 'highlighted', 'biological', 'comparing', 'playback', 'precedence', 'privileges', 'allen', 'adjust', 'flag', 'perfectly', 'configuration', 'turning', 'warned', 'echo', 'holes', 'weak', 'mysql', 'alternatives', 'pilot', 'causing', 'plugin', 'craig', 'running', 'hate', 'trojan', 'composite', 'vague', 'experiment', 'highend', 'destroy', 'survive', 'nets', 'nonetheless', 'fcc', 'mysterious', 'clearing', 'linking', 'thirdparty', 'tokens', 'technically', 'amd', 'controversial', 'catalogue', 'ships', 'invent', 'blew', 'chair', 'underlying', 'worse', 'url', 'serves', 'setting', 'computing', 'article', 'microsoft', 'fifth', 'trigger', 'judge', 'drag', 'stages', 'liable', 'winter', 'wap', 'hadnt', 'nextgeneration', 'mentioning', 'posted', 'enabled', 'chip', 'apparent', 'creative', 'permitted', 'component', 'barry', 'diff', 'hardware', 'politics', 'slightly', 'copyrighted', 'ross', 'forecast', 'jail', 'assumption', 'surfing', 'gpg', 'purely', 'deployed', 'brilliant', 'complaint', 'incoming', 'forbes', 'veteran', 'inappropriate', 'wider', 'toshiba', 'anger', 'anthony', 'mechanism', 'drivers', 'revision', 'networks', 'shell', 'script', 'altered', 'holy', 'nearby', 'implementing', 'packaged', 'communicating', 'intellectual', 'incentive', 'complaints', 'compromised', 'attempted', 'horror', 'jaguar', 'achievement', 'signals', 'disaster', 'heh', 'external', 'looks', 'developer', 'machines', 'configure', 'fault', 'attacks', 'variations', 'predicted', 'closest', 'supposedly', 'supreme', 'indians', 'mailman', 'bind', 'alike', 'continent', 'senders', 'contributor', 'theyll', 'funny', 'corpus', 'boston', 'lines', 'kick', 'announce', 'sept', 'sun', 'lcd', 'copying', 'jeremy', 'murder', 'concrete', 'physically', 'signatures', 'appeared', 'researchers', 'themes', 'poll', 'microsystems', 'scary', 'cambridge', 'suspect', 'scott', 'upgrade', 'icq', 'suits', 'bars', 'reporters', 'fan', 'sky', 'faced', 'highlighting', 'destroyed', 'conferences', 'eager', 'bay', 'species', 'giants', 'siemens', 'replied', 'jim', 'empire', 'spec', 'sets', 'downloading', 'arguments', 'bush', 'david', 'log', 'jmjmasonorg', 'reliability', 'delayed', 'ports', 'confuse', 'justify', 'affects', 'curve', 'lucas', 'consequences', 'emerged', 'boot', 'ran', 'generally', 'obvious', 'favourite', 'privileged', 'javascript', 'verizon', 'cur', 'stephen', 'rewrite', 'temperature', 'dozen', 'dave', 'picks', 'msn', 'classic', 'peters', 'connecting', 'careers', 'sharp', 'initiatives', 'slowly', 'jake', 'distributions', 'berkeley', 'muslim', 'combat', 'daughter', 'antiquity', 'ilug', 'character', 'moves', 'supplied', 'ham', 'distributed', 'popup', 'slashdot', 'shoot', 'warner', 'maintaining', 'hurt', 'tshirt', 'pioneers', 'dell', 'laptop', 'paul', 'clicks', 'counts', 'interact', 'nick', 'routing', 'unix', 'suppose', 'annoying', 'cyber', 'structures', 'indias', 'universe', 'jon', 'notebook', 'guess', 'nvidia', 'language', 'editors', 'productivity', 'visual', 'unlikely', 'shaw', 'apparently', 'device', 'tend', 'marriage', 'scale', 'hacking', 'tonight', 'temporary', 'returning', 'optimization', 'ending', 'church', 'displays', 'comic', 'beta', 'installed', 'beating', 'cindy', 'google', 'itll', 'cameras', 'thoughts', 'scope', 'andrew', 'admin', 'climate', 'humans', 'charging', 'robin', 'innovation', 'osdn', 'sucks', 'binary', 'ide', 'reproduce', 'bridge', 'suck', 'apple', 'reboot', 'reads', 'wednesday', 'mount', 'theo', 'mixed', 'objects', 'gordon', 'hosted', 'apache', 'clues', 'odd', 'modules', 'attachments', 'firewalls', 'particularly', 'string', 'fans', 'hes', 'plugins', 'presumably', 'austin', 'sony', 'mainly', 'apples', 'router', 'scenario', 'blind', 'newer', 'ought', 'hightech', 'amendment', 'suns', 'thinks', 'php', 'discussed', 'jack', 'gregory', 'stupid', 'filters', 'layer', 'weather', 'increasingly', 'resubscribe', 'greg', 'gay', 'headers', 'justin', 'architecture', 'characters', 'boomer', 'programmer', 'stage', 'engineer', 'command', 'commands', 'header', 'imho', 'comment', 'jul', 'wasnt', 'clue', 'hal', 'builtin', 'bits', 'sep', 'explains', 'visualize', 'map', 'gnome', 'thu', 'argument', 'kate', 'decline', 'chris', 'stuck', 'default', 'writes', 'wed', 'pointed', 'msg', 'silly', 'kevin', 'depends', 'compile', 'oct', 'tree', 'necessarily', 'bell', 'rebuild', 'module', 'tom', 'fork', 'folders', 'keys', 'tag', 'speech', 'apps', 'rss', 'img', 'install', 'fairly', 'mason', 'score', 'preferences', 'fri', 'yesterday', 'btw', 'tue', 'aug', 'apt', 'switch', 'cnet', 'adam', 'useless', 'murphy', 'perl', 'gary', 'cheers', 'spamassassin', 'rpm', 'wrote'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_array_words = np.append(spam_array_words, ham_array_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_ind = [tfidf.vocabulary_[word] for word in total_array_words]\n",
    "tf_select = X[:, tf_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93483469094393867"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dos = LogisticRegression()\n",
    "clf_dos.fit(tf_select, y_train)\n",
    "Y_hat = clf_dos.predict(tf_select)\n",
    "error = zero_one_loss(y_train, Y_hat)\n",
    "accuracy = 1 - error\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student",
     "written",
     "q_eda1"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the first graded EDA cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student",
     "written",
     "q_eda2"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the second graded EDA cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student",
     "written",
     "q_eda3"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the third graded EDA cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student",
     "written",
     "q_eda4"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the fourth graded EDA cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an ROC Curve\n",
    "\n",
    "It turns out that there's a tradeoff between sensitivity and specificity. In most cases we won't be able to get perfect sensitivity and specificity, so we have to select which of two we value more. For example, in the case of cancer screenings we value specificity more because false negatives are comparatively worse than false positives — a false negative means that a patient might not discover a disease until it's too late to treat, while a false positive means that a patient will probably have to take another screening.\n",
    "\n",
    "Recall that logistic regression calculates the probability that an example belongs to a certain class. Then, to classify an example we say that an email is spam if our classifier gives it >=0.5 probability of being spam. However, we can adjust that cutoff: we can say that an email is spam only if our classifier gives it >=0.7 probability of being spam, for example. This is how we can trade off sensitivity and specificity.\n",
    "\n",
    "The ROC (receiver operating charactistic) curve shows this trade off for each possible cutoff probability. We will discuss this during lecture, and you can also read [this blog post for more information.](https://www.theanalysisfactor.com/what-is-an-roc-curve/).\n",
    "\n",
    "In the light blue cell below, plot the ROC curve for your final classifier (the one you use to make predictions for Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student",
     "written",
     "q_roc"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Note that you'll want to use the .predict_proba(...) method for your classifier\n",
    "# instead of .predict(...) so you get probabilities, not classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting to Kaggle\n",
    "\n",
    "The following code will write your predictions on the test dataset to a CSV, which you can submit to Kaggle. You may need to modify it to suit your needs.\n",
    "\n",
    "The code below assumes that you've saved your predictions in a 1-dimensional array called `test_predictions`.\n",
    "\n",
    "Remember that if you've performed transformations or featurization on the training data, you must also perform the same transformations on the test data in order to make predictions. For example, if you've created features for the words \"drug\" and \"money\" on the training data, you must also extract the same features in order to use scikit-learn's `.predict(...)` method.\n",
    "\n",
    "You should submit your CSV files to https://www.kaggle.com/t/433a6bca95f94a78a0d2a6e7e8b311c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Assuming that your predictions on the test set are stored in a 1-dimensional array called\n",
    "# test_predictions. Feel free to modify this cell as long you create a CSV in the right format.\n",
    "assert isinstance(test_predictions, np.ndarray)\n",
    "assert test_predictions.shape == (1000, )\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Id\": test['id'], \n",
    "    \"Class\": test_predictions,\n",
    "}, columns=['Id', 'Class'])\n",
    "\n",
    "timestamp = datetime.isoformat(datetime.now()).split(\".\")[0]\n",
    "\n",
    "submission_df.to_csv(\"submission_{}.csv\".format(timestamp), index=False)\n",
    "print('Created a CSV file: {}.'.format(\"submission_{}.csv\".format(timestamp)))\n",
    "print('You may now upload this CSV file to Kaggle for scoring.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Run the cell below to submit your notebook to OkPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run this cell to create a PDF to upload to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student",
     "no-ok"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U gs100\n",
    "from gs100 import convert\n",
    "# Change the zoom argument if your font size is too small\n",
    "convert('proj2.ipynb', num_questions=8, zoom=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to upload your PDF now. Otherwise, your written questions won't be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
